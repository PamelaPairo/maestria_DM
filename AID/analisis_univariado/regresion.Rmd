---
title: "Regresión Lineal y Múltiple"
author: "Pamela E. Pairo"
lang: es
format:
  html:
    theme:  flatly
    code-fold: show
    code-tools: true
    toc: true
    toc-location: left
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(tidyverse)
library("readxl")
library(MASS)
library(car)
```

Se dispone de un dataset con datos del precio de viviendas de CABA y información respecto a la antiguedad y la superficie.

```{r}
casas <-read_excel("casas.xlsx")
summary(casas)
```
Se realiza un grafico de dispersión.

```{r}
p<-ggplot(casas, 
          aes(x =surface_total , y = price)) + 
          geom_point(aes(), colour ="deepskyblue", size=2)
p + xlab("Superficie") +  ylab("Precio") 
```

## Regresión Lineal Simple

Se plantea el modelo de Regresión

```{r}
modelo1<-lm(price ~ surface_total, 
            data=casas)
summary(modelo1)
```
Se calculan los residuos del modelo para chequear los supuestos.


```{r}
#Calculamos los residuos y los predichos
e<-resid(modelo1) # residuos
re<-rstandard(modelo1) #residuos estandarizados
pre<-predict(modelo1) #predichos
res<-cbind(casas$surface_total,casas$price,pre,e,round(re,2))
colnames(res)<-c("superficie", "precio", "Predichos", "Residuos", "residuos std") 
head(res)
```

Evaluamos el supuesto de normalidad de manera gráfica y mediante una prueba de hipótesis.

```{r}
#Supuestos
qqPlot(e)
```

```{r}
shapiro.test(e)
```
Como el p-valor= 0.284 es menor a $\alpha$ (0.05), se asume normalidad en los datos.

**Intervalo de Confianza**

Se calcula el intervalo de confianza del 95% para la ordenada al origen y la pendiente del modelo de regresión. 

Para cambiar el nivel de confianza cambiar el parámetro `level`

```{r}
confint(modelo1)#por default es del 95%
```
Se agrega la recta al gráfico

```{r}

p + geom_smooth(method = "lm", 
                se = TRUE)#para mostrar la banda de confianza
```

Entonces, se analiza el el modelo propuesto y como el p-valor dado es menor al valor de significancia, entonces la pendiente es significativamente distinta a cero y por ende el modelo lineal propuesto es válido.

```{r}
summary(modelo1)
```

Por el valor obtenido de $\ R^2= 0.86$ se deduce que el modelo propuesto explica el 86,63% de la variabilidad de los datos.

```{r}
#coeficiente de determinación (en summary)
summary(modelo1)$r.squared
```

## Regresión Lineal Múltiple

```{r}
modelo2<-lm(price ~ surface_total + antiguedad, 
            data=casas)
```

```{r}
a <- ggplot(casas, aes(surface_total, antiguedad))
a + geom_point()
```

```{r}
shapiro.test(resid(modelo2))
```
Vemos los resultados del modelo

```{r}
summary(modelo2)
```

```{r}
library(faraway)
prplot(modelo2,1)
```
```{r}
p + geom_smooth(method = "lm", 
                se = FALSE)
```



