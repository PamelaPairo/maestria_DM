---
title: "Análisis Discriminante"
author: "Pamela E. Pairo"
output:
  html_document:
    toc: yes
    code_folding: show
    toc_float: yes
    df_print: paged
    theme: united
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(tidyverse)
library(tidymodels)
library(factoextra)
library(mvShapiroTest)
library(biotools)
library(downloadthis)
library(MASS)
library(klaR)
```

```{r, echo = FALSE}

download_link(
  link = "https://github.com/PamelaPairo/maestria_DM/raw/main/AID/AD/analisis_discriminante.Rmd",
  button_label = "Download .Rmd",
  button_type = "danger",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```


Es una técnica de **clasificación** en donde hay más de una variable respuesta (cuantitativa) medida en grupos predeterminados.

El objetivo es:

- Separar los grupos, identificando las variables que permiten esa separación

- Clasificar a nuevos individuos

# Base de datos

Vamos a trabajar con la base de datos de `Iris`


```{r}
data(iris)
glimpse(iris)
```
Realicemos algunos scatterplots

```{r}
library(GGally)

ggpairs(iris, legend = 1, columns = 1:4, aes(color = Species, alpha = 0.5),
        upper = list(continuous = "points"))+
  theme(legend.position = "bottom")
```

Al conocerse la especie de pertenencia de cada individuo, se puede comprobar la efectividad del método de clasificación observando el porcentaje de casos bien clasificados. Para ello, vamos a separar el dataset en train y test

```{r}
set.seed(123)#setear la semilla
# Create data split for train and test
df_split <- initial_split(iris,
                          prop = 0.9,
                          strata = Species)#en este caso no es necesario porque las clases están balanceadas

df_train <- df_split %>%
              training()

df_test <- df_split %>%
              testing()

# Número de datos en test y train
paste0("Total del dataset de entrenamiento: ", nrow(df_train))
paste0("Total del dataset de testeo: ", nrow(df_test))
```

# Análisis Discriminante Lineal (LDA)

Este tipo de análisis es válido solo si se satisfacen los siguientes supuestos:

1- Normalidad multivariada

2- Independencia de las observaciones

3- Homocedasticidad.

## Verefiquemos supuestos

1- Normalidad multivariada

¿Cuál es la hipótesis estadística?

```{r}
mvShapiro.Test(as.matrix(df_train[,-5]))
```

2- Independencia de las observaciones

Viene dada por el diseño

3- Homocedasticidad

Ho las matrices de varianzas-covarianzas de los grupos son iguales.

Analizamos igualdad de matrices de varianzas y covarianzas:

```{r}
boxM(df_train[,-5],df_train[,5])
```
Entonces ¿qué concluimos?

Vamos a proseguir como si se hubiese cumplido todos los supuestos. Tener en cuenta que los resultados del LDA **no van a ser confiables en este caso**.

## LDA

```{r}
model_lda <- lda(Species~., data =df_train)
model_lda
```
El objeto lda (en este ejemplo, model_lda) contiene los siguientes componentes:

*prior*: las probabilidades previas utilizadas.

*means*: la media de cada clase.

*svd*: son los valores singulares, informan el cociente entre desvíos estándar entre y dentro de grupos para cada FD; si se elevan al cuadrado se obtiene el autovalor de cada FD

*counts*: El número de observaciones por clase.

**Coefficients of linear discriminants**: Muestra la combinación lineal de variables predictoras que se utilizan para formar la regla de decisión LDA.

Por ejemplo:

$\ LD1= 0.76*Sepal.Length+ 1.85*Sepal.Width - 2.25*Petal.Length -2.91*Petal.Width$

LD1 explica el 99% de la proporción de varianza entre clases.

La 2da función discriminante es independiente de la primera (ortogonal) y es la que mejor separa los grupos usando la variación remanente o residual, después que la 1ra función discriminante ha sido determinada

```{r}
prop = model_lda$svd^2/sum(model_lda$svd^2)
prop #varianza entre grupos explicada por cada FD
```

```{r}
lda.data <- cbind(df_train, predict(model_lda)$x)
ggplot(lda.data, aes(LD1, LD2)) +
  geom_point(aes(color = Species))
```

Veamos que predice el modelo en df_test

```{r}
predictions <- model_lda %>% predict(df_test)
predictions
```

Veamos la matriz de confusión en df_train

```{r}
table(predict(model_lda,type="class")$class,df_train$Species)
```
`partimat` muestra una matriz de gráficos para cada combinación de dos variables. Cada gráfico muestra una vista diferente de los mismos datos. Las regiones coloreadas delimitan cada área de clasificación. Se predice que cualquier observación que se encuentre dentro de una región pertenece a una clase específica. Cada gráfico también incluye la tasa de error aparente para esa vista de los datos.

```{r}
library(klaR) 
partimat (Species~. , data=df_train , method="lda")
```

Veamos la _performance_ en df_test

```{r}
lda.test <- predict(model_lda,df_test)
df_test$lda <- lda.test$class
table(df_test$lda,df_test$Species)
```
### Biplot

```{r}
#install.packages("devtools")
#library(devtools)
#install_github("fawda123/ggord")
library(ggord)
ggord(model_lda, df_train$Species, xlim = c(-10, 11))
```

# Análisis discriminante cuadrático (QDA)

El discriminante se dice cuadrático porque el término de segundo orden no se cancela como en el caso del discriminante lineal. QDA no asume la igualdad en la matriz de varianzas/covarianzas. En otras palabras, para QDA la matriz de covarianza puede ser diferente para cada clase.

Vamos a aplicar QDA, a pesar de que no se satisface el supuesto de normalidad multivariada.

```{r}
model_qda <- qda(Species ~ ., df_train)
model_qda
```

```{r}
partimat(Species ~ ., data=df_train, method="qda")
```

```{r}
table(predict(model_qda,type="class")$class,df_train$Species)
```

```{r}
lda.test_qda <- predict(model_qda,df_test)
df_test$qda <- lda.test_qda$class
table(df_test$qda,df_test$Species)
```
